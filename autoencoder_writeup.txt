An autoencoder is a simple neural network that is designed to learn a representation of the input. What this means is that its output is the same number of nodes as its input, and what the autoencoder does is try to reconstruct the input in its output, and thus as it learns the hidden layer neurons learn a functional representation of the input to be able to reconstruct it.

Structurally an autoencoder is very similar to our multilayer perceptron, where it has as an input layer, a hidden layer, and an output layer. The difference is that the input layer and the output layer have the same number of nodes, the output layer retransforms the internal representation into the form of the input, and the loss function that is being optimized is the average reconstruction error.

In the case of our autoencoder, the average reconstruction error loss function is given by the reconstruction cross-entropy: 
(copy function from Theano tutorial)

Intuitively, if we have less hidden nodes than input/output nodes, what the autoencoder is doing is learning a compact representation of the features that it needs to model the input in an unsupervised way. This has various use cases such as image compression, etc, but our use case was to take the trained autoencoder and remove the output layer, and then use the weights of the autoencoder's hidden layer as initialized weights of a multilayer perceptron's hidden layer. The idea is that the weights pretrained through the autoencoder already know something about the problem of digit recognition, and so they will start the classification problem at a lower error rate, and the goal is that they will converge faster to a possibly better local optima than one that would be reached from random initial weights. This idea was taken to multiple levels in our experiments by adding layers on top of the autoencoder's hidden layer.

Additionally, we experimented with denoising autoencoders, which are the same idea as regular autoencoders as described above, except that you add a level of noise to the input data as it goes into the hidden layer, and attempt to reconstruct the original uncorrupted image. What this does is prevent overfitting and intuitively force the hidden layer to represent the important and general features of the data, so that it can still reproduce images without noise. We ran experiments with pretraining on both regular autoencoders with uncorrupted data and denoising autoencoders with 30% corrupted data.

